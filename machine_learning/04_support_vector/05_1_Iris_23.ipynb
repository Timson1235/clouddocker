{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Different SVMs on IRIS Data\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Content:</b> In this notebook, we use the IRIS Dataset (or rather a subset of it) to demonstrate different SVMs.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.svm import SVC \n",
    "import time \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Loading the Dataset\n",
    "* Iris data is part of a couple of libraries, among them Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "The data has four features, thus we cannot visualize it directly. Therefore, we compare pairs of features in the pairplot.\n",
    "We can observe, that among the three species, the members of setosa form an isolated cluster, whereas versicolor and virginica are not as clearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = iris.species\n",
    "X = iris.drop('species',axis=1)\n",
    "sns.pairplot(iris, hue=\"species\",palette=\"bright\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## SVM Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Data preprocessing\n",
    "* For the sake of simplicity in this demo, we only consider a two-class problem: setosa vs. versicolor. The SVM implementation can cover multidimensional problems as well (tbd later during the term).\n",
    "* Also, we want to be able to visualize the data and the resulting SVM directly. Therefore, we only use two features and drop the others.\n",
    "* We replace the species by numerical values, as that is what the SVM can work with.\n",
    "* Finally, we split the data into features X and classes y.\n",
    "\n",
    "Let's start with a separable case (look at the pairplots!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=iris[(iris['species']!='virginica')]\n",
    "\n",
    "df=df.drop(['sepal_length','sepal_width'], axis=1)\n",
    "\n",
    "# used to automatically downcast, but will be removed in future versions (like here, casting to object)\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df=df.replace('setosa', -1)\n",
    "df=df.replace('versicolor', +1)\n",
    "df[\"species\"] = pd.to_numeric(df[\"species\"])\n",
    "\n",
    "X=df.iloc[:,0:2]\n",
    "y=df['species']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Creating Visuals\n",
    "To investigate SVMs and also to support communication of our results (e.g. in a company to co-workers, custormers, superiors, ...) we visualize our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(['dodgerblue', 'darkorange'])\n",
    "\n",
    "# Plot the dataset together with additional content and store under the given filename\n",
    "# X the dataset\n",
    "# y the labels\n",
    "# content_func a function that is executed to plot specific content into the otherwise standardized diagram\n",
    "def plot_svm(X, y, content_func):\n",
    "    X_0=X.iloc[:, 0] # values in dim 0\n",
    "    X_1=X.iloc[:, 1] # values in dim 1\n",
    "\n",
    "    f = plt.figure(figsize=(15, 15))\n",
    "    ax = plt.gca()\n",
    "    scatter=ax.scatter(X_0, X_1, c=y, s=50, cmap=cmap, label=y)\n",
    "    \n",
    "    ax.set_aspect('equal', 'box') # otherwise our linear classifier will look warped\n",
    "    ax.set_xlim(0,math.ceil(1.1*X_0.max()))\n",
    "    ax.set_ylim(0,math.ceil(1.1*X_1.max()))  \n",
    "    ax.set_xlabel(r\"$x_1$\")\n",
    "    ax.set_ylabel(r\"$x_2$\")\n",
    "    \n",
    "    content_func(ax)\n",
    "    \n",
    "    L=ax.legend(*scatter.legend_elements(),\n",
    "                    bbox_to_anchor=(1,0.4), loc=\"center right\", title=\"classes\", ncol=1)\n",
    "    L.get_texts()[1].set_text('+1') # special handling of +1 in the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def no_content(ax):\n",
    "    pass\n",
    "plot_svm(X, y, no_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Learning SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Describe the properities of an SVM\n",
    "# clf the classifier\n",
    "# X features\n",
    "# y labels\n",
    "# param_name the parameter we investigate (e.g. with different values)\n",
    "# param the parameter value\n",
    "def describe_svm(clf, X, y, param_name, param):\n",
    "    clf.fit(X, y)\n",
    "    sv=clf.support_vectors_\n",
    "    y_pred=clf.predict(X)\n",
    "    acc=round(metrics.accuracy_score(y, y_pred),5)\n",
    "    print(f\"{param_name}: \", param, \"   svs: \", len(sv), \"   train-acc: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Hard-Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "We start with a linear SVM. There is no implementation for hard-margin SVMs. But we can punish digressions so hard, that the trade-off is biased towards margin rather than digression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear', C=1E2)\n",
    "model.fit(X, y)\n",
    "describe_svm(model, X, y, \"C\", 1E2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Let's investigate our model, what does the linear classifier look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w=model.coef_[0]\n",
    "w_0=w[0]\n",
    "w_1=w[1]\n",
    "b=model.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_alpha=model.dual_coef_ # c_alpha[0,i] = c_i*alpha_i\n",
    "sv=model.support_vectors_\n",
    "\n",
    "print(np.round(w,8))\n",
    "print(c_alpha[0,0]*sv[0] + c_alpha[0,1]*sv[1]) \n",
    "# as expected, w is a linear combination of support vectors\n",
    "# (their actually computed values are different in the 16th digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hard_margin_svm(ax):\n",
    "    x_space = np.linspace(0, ax.get_xlim(), 30)\n",
    "    yc_best=-w_0/w_1*x_space-b/w_1\n",
    "    plt.plot(x_space, yc_best, color='black', alpha=1, linestyle='-')\n",
    "\n",
    "plot_svm(X, y, hard_margin_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Soft-Margin\n",
    "We switch to another subset of the data, this time creating a non-separable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=iris[(iris['species']!='setosa')]\n",
    "\n",
    "df=df.drop(['sepal_length','sepal_width'], axis=1)\n",
    "\n",
    "df=df.replace('virginica', 1)\n",
    "df=df.replace('versicolor', -1)\n",
    "df[\"species\"] = pd.to_numeric(df[\"species\"])\n",
    "\n",
    "df=df.drop_duplicates()\n",
    "df=df.drop(126) # this item creates contradictory data and thus an additional support vector. Just comment this line and see below\n",
    "\n",
    "X=df.iloc[:,0:2]\n",
    "y=df['species']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def no_content(ax):\n",
    "    pass\n",
    "plot_svm(X, y, no_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Let's try the same SVM for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = SVC(kernel='linear', C=1E2)\n",
    "model1.fit(X, y)\n",
    "describe_svm(model1, X, y, \"C\", 1E2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1=model1.coef_[0]\n",
    "w1_0=w1[0]\n",
    "w1_1=w1[1]\n",
    "b1=model1.intercept_[0]\n",
    "w1_0, w1_1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = SVC(kernel='linear', C=1E-1)  # for soft-margin\n",
    "model2.fit(X, y)\n",
    "describe_svm(model2, X, y, \"C\", 1E-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2=model2.coef_[0]\n",
    "w2_0=w2[0]\n",
    "w2_1=w2[1]\n",
    "b2=model2.intercept_[0]\n",
    "w2_0, w2_1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def two_svms(ax):\n",
    "    x_space = np.linspace(0, ax.get_xlim(), 30)\n",
    "    yc1=-w1_0/w1_1*x_space-b1/w1_1\n",
    "    yc2=-w2_0/w2_1*x_space-b2/w2_1\n",
    "\n",
    "    plt.plot(x_space, yc1, color='black', alpha=1, linestyle='-')\n",
    "    plt.plot(x_space, yc2, color='green', alpha=1, linestyle='-')\n",
    "\n",
    "plot_svm(X, y, two_svms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Finding a good C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Cs = (0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1,2,10,20,100,200)\n",
    "support_vectors=[]\n",
    "margin=[]\n",
    "train_accs=[]\n",
    "\n",
    "for C in Cs:\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    support_vectors.append(len(clf.support_vectors_))\n",
    "\n",
    "    w=clf.coef_[0]\n",
    "    np.dot(w,w)\n",
    "    margin.append(np.dot(w, w)**(-.5))\n",
    "\n",
    "    y_pred=clf.predict(X)\n",
    "    train_accs.append(round(metrics.accuracy_score(y, y_pred),5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].plot(Cs, margin)\n",
    "ax[0].set_ylabel('margin')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].plot(Cs, support_vectors)\n",
    "ax[1].set_ylabel('# svs')\n",
    "ax[2].set_xscale('log')\n",
    "ax[2].plot(Cs, train_accs) \n",
    "ax[2].set_ylabel('train-acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "* C low -> focus on margin, allow a lot of digressions -> many svs, low training-acc \n",
    "* C high -> better fit to the training data, less digressions, but also smaller margin -> __needs to be tested using previously unseen test data!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Kernel Tricks\n",
    "Now we investigate the use of kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from classification_viz import plot_decisions_2d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_svm(clf, X, y, param_name, param):\n",
    "    describe_svm(clf, X, y, param_name, param)\n",
    "    plot_decisions_2d(X, y, clf)    \n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for degree in (1, 2, 3, 4, 5):\n",
    "    clf = SVC(kernel='poly', degree=degree, gamma=2, coef0=1, C=1, tol=1E-5)\n",
    "    evaluate_svm(clf, X.values, y, param_name=\"degree\", param=degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Radial Basis Function Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for gamma in (0.01, 0.1, 0.5, 1, 10, 100, 1000):\n",
    "    clf = SVC(kernel='rbf', gamma=gamma, C=1, tol=1E-5)\n",
    "    evaluate_svm(clf, X.values, y, param_name=\"gamma\", param=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Take Aways:</b> \n",
    "\n",
    "* SVMs for binary classification.\n",
    "* SVMs with (almost) hard and soft margin\n",
    "* The Kernel Trick\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Play with:</b> \n",
    "    \n",
    "* Use the digits dataset (from our walkthrough in the beginning)\n",
    "* Conduct similar experiments and observe the results of different kernels.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envML24sose",
   "language": "python",
   "name": "envml24sose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
